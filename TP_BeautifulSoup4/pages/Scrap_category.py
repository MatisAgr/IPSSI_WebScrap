import streamlit as st
import sys
import os
from pymongo.errors import PyMongoError
import requests
import re # Importer re pour la mise en page

# --- Configuration et Imports ---
# Assurez-vous que le r√©pertoire parent est dans le path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.append(parent_dir)

import TP_BeautifulSoup4 as scraper # Importe depuis le r√©pertoire parent
import mongo_connect as db_connector # Importe depuis le r√©pertoire parent

# --- Constantes ---
BASE_URL = "https://www.blogdumoderateur.com/"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# --- Fonction pour r√©cup√©rer les URLs des cat√©gories (avec cache) ---
@st.cache_data(ttl=3600) # Cache les r√©sultats pendant 1 heure
def get_category_urls():
    """R√©cup√®re les URLs des cat√©gories depuis le site."""
    print("Attempting to fetch category URLs...") # Pour le d√©bogage
    urls = scraper.scrape_category_urls(BASE_URL, HEADERS)
    print(f"Fetched URLs: {urls}") # Pour le d√©bogage
    return urls

# --- Initialiser l'√©tat de session ---
if 'category_articles_to_display' not in st.session_state:
    st.session_state.category_articles_to_display = None
if 'category_url_processed' not in st.session_state:
    st.session_state.category_url_processed = ""
if 'selected_category_url' not in st.session_state:
     st.session_state.selected_category_url = None # Pour stocker la s√©lection

# --- Titre sp√©cifique √† la page ---
st.title("üóÇÔ∏è Scraper une Cat√©gorie d'Articles")
st.write("Choisissez une cat√©gorie dans la liste ci-dessous pour scraper tous les articles list√©s sur cette page.")

# --- R√©cup√©rer les URLs des cat√©gories ---
category_urls = get_category_urls()

# --- Afficher la liste d√©roulante ou un message d'erreur ---
if category_urls:
    # Cr√©er une liste d'options pour le selectbox (URL -> Nom lisible si possible)
    # Ici, on utilise juste les URLs comme options pour simplifier
    # On pourrait extraire le nom de la cat√©gorie de l'URL ou du texte du lien si scrape_category_urls le retournait
    selected_category_url = st.selectbox(
        "Choisissez une cat√©gorie √† scraper :",
        options=category_urls,
        index=None, # Ne pr√©s√©lectionne rien
        placeholder="S√©lectionnez une cat√©gorie...",
        key="select_category_url" # Cl√© unique pour le widget
    )
    st.session_state.selected_category_url = selected_category_url # Mettre √† jour l'√©tat
else:
    st.error("Impossible de r√©cup√©rer la liste des cat√©gories depuis le site. V√©rifiez la console ou r√©essayez plus tard.")
    st.stop() # Arr√™te l'ex√©cution du script si les cat√©gories ne peuvent pas √™tre charg√©es

# --- Bouton pour lancer le scraping (maintenant bas√© sur la s√©lection) ---
# D√©sactiver le bouton si aucune cat√©gorie n'est s√©lectionn√©e
scrape_category_button = st.button(
    "Scraper cette Cat√©gorie",
    key="scrape_category_button",
    disabled=not st.session_state.selected_category_url # D√©sactiv√© si rien n'est s√©lectionn√©
)

# --- Logique de scraping (utilise st.session_state.selected_category_url) ---
if scrape_category_button and st.session_state.selected_category_url:
    selected_url = st.session_state.selected_category_url
    # La validation startswith n'est plus n√©cessaire car les URLs viennent de notre scraping
    with st.spinner(f"Scraping de la cat√©gorie : {selected_url}..."):
        try:
            # Utilise la fonction qui r√©cup√®re aussi les d√©tails (summary, author, etc.)
            articles_data = scraper.scrape_article_previews(selected_url)

            if not articles_data:
                st.warning("Aucun article trouv√© sur cette page ou erreur lors du scraping.")
                st.session_state.category_articles_to_display = None
                st.session_state.category_url_processed = ""
            else:
                st.success(f"Scraping termin√© ! {len(articles_data)} article(s) trouv√©(s).")
                st.session_state.category_articles_to_display = articles_data
                st.session_state.category_url_processed = selected_url # Stocker l'URL trait√©e

        except requests.exceptions.RequestException as e:
            st.error(f"Erreur de requ√™te lors du scraping : {e}")
            st.session_state.category_articles_to_display = None
            st.session_state.category_url_processed = ""
        except Exception as e:
            st.error(f"Erreur inattendue lors du scraping : {e}")
            st.session_state.category_articles_to_display = None
            st.session_state.category_url_processed = ""

# --- Affichage des r√©sultats ET bouton de sauvegarde (bas√© sur l'√©tat de session) ---
if st.session_state.category_articles_to_display:
    articles_data = st.session_state.category_articles_to_display
    st.subheader(f"Articles trouv√©s pour : {st.session_state.category_url_processed}")
    st.info(f"{len(articles_data)} article(s) trouv√©(s). Pr√™t(s) √† √™tre sauvegard√©(s).")

    # --- D√©but de la mise en page similaire √† IHM_mongo.py ---
    num_columns = 2 # Ou 1 si vous pr√©f√©rez une seule colonne
    cols = st.columns(num_columns)
    for i, article in enumerate(articles_data):
        col_index = i % num_columns
        with cols[col_index]:
            with st.container(border=True):
                # Titre cliquable
                if article.get("title") and article.get("url"):
                    st.subheader(f"[{article['title']}]({article['url']})")
                elif article.get("title"):
                    st.subheader(article['title'])

                # Miniature
                if article.get("thumbnail"):
                    st.image(article["thumbnail"], use_column_width=True)

                # M√©tadonn√©es (Auteur, Date, Tags)
                meta_info = []
                if article.get("author"): meta_info.append(f"üë§ {article['author']}")
                if article.get("date_iso"): meta_info.append(f"üìÖ {article['date_iso']}")
                elif article.get("date_display"): meta_info.append(f"üìÖ {article['date_display']}")
                if article.get("tags"): # V√©rifie si la cl√© 'tags' existe et si la liste n'est pas vide
                    tags_str = ", ".join(article["tags"]) # Joint les tags en une seule cha√Æne
                    meta_info.append(f"üè∑Ô∏è Tags: {tags_str}") # Ajoute la cha√Æne format√©e
                if meta_info: st.caption(" | ".join(meta_info))

                # R√©sum√©
                if article.get("summary"):
                    with st.expander("R√©sum√©"): st.write(article["summary"])

                # Bouton Lire l'article
                if article.get("url"): st.link_button("Lire l'article ‚ÜóÔ∏è", article["url"])

                # Images du contenu
                if article.get("content_images"):
                     with st.expander(f"{len(article['content_images'])} image(s) dans le contenu"):
                         for img_data in article["content_images"]:
                             st.image(img_data.get("url"), caption=img_data.get("caption_or_alt", ""), use_column_width=True)
    # --- Fin de la mise en page ---

    st.divider() # Garder le s√©parateur avant le bouton de sauvegarde

    # Bouton de sauvegarde (reste inchang√©)
    if st.button(f"Sauvegarder les {len(articles_data)} articles dans MongoDB", key="save_category_articles"):
        collection = db_connector.connect_to_mongo()
        if collection is not None:
            inserted_count = 0
            skipped_count = 0
            error_count = 0
            with st.spinner("Sauvegarde des articles en cours..."):
                for article in articles_data:
                    try:
                        # V√©rifier si l'article existe d√©j√† par URL
                        if article.get('url'):
                            existing = collection.find_one({'url': article['url']})
                            if existing:
                                skipped_count += 1
                                continue # Passer au suivant
                        # Ins√©rer s'il n'existe pas ou si l'URL manque
                        collection.insert_one(article)
                        inserted_count += 1
                    except PyMongoError as e:
                        st.warning(f"Erreur MongoDB lors de la sauvegarde de '{article.get('title', 'N/A')}': {e}")
                        error_count += 1
                    except Exception as e:
                        st.warning(f"Erreur inattendue lors de la sauvegarde de '{article.get('title', 'N/A')}': {e}")
                        error_count += 1

            st.success(f"Sauvegarde termin√©e : {inserted_count} articles ins√©r√©s.")
            if skipped_count > 0:
                st.info(f"{skipped_count} articles d√©j√† pr√©sents ont √©t√© ignor√©s.")
            if error_count > 0:
                st.error(f"{error_count} erreurs rencontr√©es lors de la sauvegarde.")
            # Optionnel : R√©initialiser apr√®s sauvegarde pour √©viter re-sauvegarde accidentelle
            # st.session_state.category_articles_to_display = None
            # st.session_state.category_url_processed = ""
            # st.session_state.selected_category_url = None # R√©initialiser la s√©lection aussi
            # st.rerun()
        else:
            st.error("Connexion √† MongoDB √©chou√©e, impossible de sauvegarder.")
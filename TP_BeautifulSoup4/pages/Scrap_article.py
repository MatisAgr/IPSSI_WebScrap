import streamlit as st
import TP_BeautifulSoup4 as scraper # Importe le module de scraping
import requests # N√©cessaire pour d√©finir les headers

# --- Titre sp√©cifique √† la page ---
st.title("üöÄ Scraper un Article Sp√©cifique")
st.write("Entrez l'URL d'un article du Blog du Mod√©rateur pour en extraire les informations.")

# --- Champ d'entr√©e pour l'URL ---
article_url = st.text_input("URL de l'article √† scraper", placeholder="https://www.blogdumoderateur.com/...")

# --- Bouton pour lancer le scraping ---
scrape_button = st.button("Scraper cet Article")

# --- Logique de scraping et affichage ---
if scrape_button and article_url:
    if not article_url.startswith("https://www.blogdumoderateur.com/"):
        st.warning("Veuillez entrer une URL valide commen√ßant par 'https://www.blogdumoderateur.com/'")
    else:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        with st.spinner(f"Scraping de l'article : {article_url}..."):
            try:
                # Note: Nous supposons que scrape_article_details r√©cup√®re TOUTES les infos
                # Si ce n'est pas le cas, il faudra adapter TP_BeautifulSoup4.py
                # ou cr√©er une fonction d√©di√©e pour scraper *tous* les champs d'un seul article.
                # Pour l'instant, on utilise la fonction existante.
                article_data = scraper.scrape_article_details(article_url, headers)

                if not article_data or (article_data.get('summary') is None and article_data.get('author') is None):
                     st.error("Impossible de scraper les d√©tails de cet article. V√©rifiez l'URL ou la structure de la page.")
                else:
                    st.success("Scraping termin√© !")

                    # Afficher les informations r√©cup√©r√©es
                    st.subheader(f"D√©tails pour : {article_url}") # On n'a pas le titre ici, sauf si on modifie la fonction

                    if article_data.get("author"):
                        st.write(f"üë§ **Auteur:** {article_data['author']}")
                    # On n'a pas la date ou la cat√©gorie avec la fonction actuelle
                    # if article_data.get("date_iso"): st.write(f"üìÖ **Date:** {article['date_iso']}")
                    # if article_data.get("subcategory"): st.write(f"üè∑Ô∏è **Cat√©gorie:** {article['subcategory']}")

                    # Afficher le r√©sum√©
                    if article_data.get("summary"):
                        st.write("**R√©sum√© (Chapeau) :**")
                        st.info(article_data["summary"])
                    else:
                        st.warning("R√©sum√© non trouv√©.")

                    # Afficher les images du contenu
                    content_images = article_data.get("content_images", [])
                    if content_images:
                        st.write(f"**{len(content_images)} Image(s) trouv√©e(s) dans le contenu :**")
                        for img_data in content_images:
                            st.image(img_data.get("url"), caption=img_data.get("caption_or_alt", ""), use_column_width=True)
                    else:
                        st.write("Aucune image trouv√©e dans le contenu.")

                    # Optionnel : Bouton pour sauvegarder dans MongoDB
                    # N√©cessiterait d'importer mongo_connect et d'ajouter la logique d'insertion ici
                    # if st.button("Sauvegarder cet article dans MongoDB"):
                    #     collection = db_connector.connect_to_mongo()
                    #     if collection is not None:
                    #         try:
                    #             # Il faudrait r√©cup√©rer TOUTES les donn√©es (titre, thumb, etc.) pour que ce soit utile
                    #             # insert_result = collection.insert_one(article_data_complet)
                    #             # st.success(f"Article sauvegard√© avec l'ID: {insert_result.inserted_id}")
                    #             st.warning("Fonctionnalit√© de sauvegarde non impl√©ment√©e avec les donn√©es actuelles.")
                    #         except Exception as e:
                    #             st.error(f"Erreur lors de la sauvegarde : {e}")
                    #     else:
                    #         st.error("Connexion √† MongoDB √©chou√©e, impossible de sauvegarder.")


            except requests.exceptions.RequestException as e:
                st.error(f"Erreur de requ√™te lors du scraping : {e}")
            except Exception as e:
                st.error(f"Erreur inattendue lors du scraping : {e}")

elif scrape_button and not article_url:
    st.warning("Veuillez entrer une URL.")
